{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "U4W22_53_Siamese_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramchand32/learngithub/blob/main/U4W22_53_Siamese_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY8HfxCh_CSO"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRRaLUgO81jh"
      },
      "source": [
        "### Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnrTzEfE9dgm"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* Understand the challenges of facial recognition and the IMFDB dataset\n",
        "* Be able to use the Siamese network to get similarity score between images\n",
        "* Classify the IMFDB dataset using MLP by using the features obtained from Siamese Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EMjDna1Kxgt",
        "cellView": "form"
      },
      "source": [
        "#@title Experiment Explanation Video\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<video width=\"850\" height=\"480\" controls>\n",
        "  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/Apr15/siamese_exp_walkthrough2.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K38V-8qoKqfq"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdToIRQ9v-U"
      },
      "source": [
        "#### Description\n",
        "\n",
        "Indian Movie Face database (IMFDB) is a large unconstrained face database **consisting of 34512 images of 100 Indian actors collected from more than 100 videos. (For this experiment we are using data of 16 actors, which results in 5000 training samples and 1095 testing samples.)**  \n",
        "* Faces in IMFDB are collected from Indian movie videos. Faces in movie videos arguably have large variations in scale, pose, expression, illumination, age, resolution, occlusion, and makeup.\n",
        "* Videos collected from the last two decades contain large diversity in age variations compared to the images collected from the Internet through a search query.\n",
        "* Public figures often retain certain kinds of appearance, dress patterns, and expressions when appearing in public while movie videos offer greater variations.\n",
        "* IMFDB is built by manual selection and cropping of video frames resulting in a large spectrum of poses to develop efficient algorithms to handle pose.\n",
        "\n",
        "The image below contains few sample faces from the dataset:\n",
        "\n",
        "<img src = \"http://cvit.iiit.ac.in/projects/IMFDB/faces5.png\">\n",
        "\n",
        "Design guidelines followed in the database are:\n",
        "\n",
        "* **Selection of movies and actors**: To ensure diversity in the appearance of actors, movies are selected from 5 Indian languages namely, Hindi, Telugu, Kannada, Malayalam, and Bengali. For each actor, movies are selected such that they give wide variations in age.\n",
        "* **Selection of frames**:  Often, a single frame is selected from a shot (scene) unless there is another frame with a significant difference from the previously selected frame. If there are multiple variations available in a shot, face with occlusion and pose variation, were preferred. Small faces that are difficult to recognize manually are not considered.\n",
        "* **Cropping of faces**: Faces are cropped with a tight bounding box. To maintain consistency across images, we followed a heuristic of cropping the face from forehead to chin.\n",
        "* **Annotation**: For every image, annotation is provided for the following attributes:\n",
        "\n",
        "> \n",
        "    Expressions     :  Anger, Happiness, Sadness, Surprise, Fear, Disgust\n",
        "    Illumination    :  Bad, Medium, High\n",
        "    Pose            :  Frontal, Left, Right, Up, Down \n",
        "    Occlusion       :  Glasses, Beard, Ornaments, Hair, Hand\n",
        "    Age             :  Child, Young, Middle, and Old\n",
        "    Makeup          :  Partial makeup, Over-makeup\n",
        "    Gender          :  Male, Female\n",
        "\n",
        "*Source: Shankar Setty, Moula Husain, Parisa Beham, Jyothi Gudavalli, Menaka Kandasamy, Radhesyam Vaddi, Vidyagouri Hemadri, J C Karure, Raja Raju, Rajan, Vijay Kumar and C V Jawahar. \"Indian Movie Face Database: A Benchmark for Face Recognition Under Wide Variations\"\n",
        "National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG), 2013.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtRlKzFg90Ec"
      },
      "source": [
        "#### Challenges\n",
        "Face recognition systems yield satisfactory performance only under controlled scenarios and they degrade significantly when confronted with real-world scenarios.\n",
        "\n",
        "The real-world scenarios have unconstrained conditions such as illumination and pose variations, occlusion, and expressions. These have to be considered while building the dataset and developing facial recognition systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRcNSMp6Lhjl"
      },
      "source": [
        "### Domain Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezpOrYbJ9jYM"
      },
      "source": [
        "\n",
        "Face recognition is one of the most popular applications of image analysis software today. \n",
        "\n",
        "For facial recognition software to identify unique facial features, it has to perform a number of tasks. \n",
        "\n",
        "**Face Recognition is the Last Step**\n",
        "\n",
        "* **Face detection**: first, the system has to identify the part of the image or the video that represents the face.\n",
        "* **Pre-processing**: the data has to be transformed into a normalized. It is also often referred to as feature normalization.\n",
        "* **Feature extraction**: the system has to extract meaningful data from the facial images, identifying the most relevant bits of data and ignoring all of the “noise.” It’s also referred to as encoding.\n",
        "* **Face recognition**: the actual process of matching unique data features to each individual. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecb84yA2984p"
      },
      "source": [
        "### AI/ML Technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1P4t0xGAtGe"
      },
      "source": [
        "#### Siamese Network for Image Classification: \n",
        "\n",
        "In this experiment, we use the IMFDB dataset for face recognition. Firstly, Siamese Network identifies similarities between images. Secondly, the various images can be classified by a Multilayer Perceptron (MLP).\n",
        "\n",
        "**Siamese Network** is a special type of neural network architecture. Instead of a model learning to classify its inputs, this neural network learns to **differentiate** between two inputs. It learns the **similarity** between them.\n",
        "\n",
        "There are different classification models used for image classification. In this experiment, one-shot classification model is used to perform the classification.\n",
        "\n",
        "**One-Shot Classification models** require that you have just one training example of each class you want to predict on. The model is still trained on several instances, but they only have to be in a similar domain as your training example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHbq_76zCFsx"
      },
      "source": [
        "#### Siamese Network Architecture\n",
        "\n",
        "It consists of two identical neural networks (sister networks), each taking one of the two input images. The last layers of the two networks are then fed to a contrastive loss function, which calculates the similarity between the two images. \n",
        "\n",
        "The image below helps us understand the above:\n",
        "\n",
        "<img src = \"https://cdn-images-1.medium.com/max/600/1*XzVUiq-3lYFtZEW3XfmKqg.jpeg\" width=\"400\" height = \"500\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuObTdeFBiGY"
      },
      "source": [
        "Tons of data are available on the web (Wikipedia, Google, Twitter, YouTube) that can be used to train an ML model. One such source is Google Images. You enter a text query and Google Images show thousands of related images based on the query and text that are present on the web page with the related image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhoMm0ENB04C"
      },
      "source": [
        "### Setup Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFM_mHXUr0m5"
      },
      "source": [
        "#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2201505\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-UzlZNwr9vs"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"8971446287\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fed0e9f-d03e-4c75-dbd4-bfa7571cd249"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"U4W22_53_Siamese_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/One_shot_Face_recognition.zip\")\n",
        "    ipython.magic(\"sx unzip -qq One_shot_Face_recognition.zip\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aiml.iiith.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "def getWalkthrough():\n",
        "  try:\n",
        "    if not Walkthrough:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Walkthrough\n",
        "  except NameError:\n",
        "    print (\"Please answer Walkthrough Question\")\n",
        "    return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2201505&recordId=10110\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSVGip5-LmQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b6bf8c-98d7-4efa-e60d-b20cb0c946d0"
      },
      "source": [
        "# Change the current working directory to the One_shot_Face_recognition folder, which contains all of the files.\n",
        "%cd One_shot_Face_recognition "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/One_shot_Face_recognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzyIGz5pcQRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ab3928-2900-45a2-b580-f30f70ba5d53"
      },
      "source": [
        "# List Files in the present working directory (One_shot_Face_recognition)\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contrastive.py  \u001b[0m\u001b[01;34mdata\u001b[0m/  data_loader.py  light_cnn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq9clcuMCRVy"
      },
      "source": [
        "### Importing the Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APSGlS_s_CSU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models \n",
        "\n",
        "# Importing python packages\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oia-EoIHCjo0"
      },
      "source": [
        "### Loading the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ISpykBYsiq9"
      },
      "source": [
        "train_list_file = 'data/IMFDB_train_sorted.txt'   # 5000 images for training\n",
        "test_list_file = 'data/IMFDB_test_sorted.txt'     # 1095 images for validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEByElGbsk4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df44b5d0-352c-4b53-a013-70c23d37d5b6"
      },
      "source": [
        "# The traditional dataloader returns images with its respective labels for each batch\n",
        "# However, the Siamese network takes in 2 images and feeds them into 2 CNNs with the same structure.\n",
        "# Henceforth, Creating and using a custom dataloader \"siamese_data_loader\", which returns pair of images and their respective label for each batch\n",
        "# i.e, \"siamese_data_loader\" gives the image pairs (image_1, image_2) and a label as input to the siamese networks.\n",
        "# see data_loader.py for details\n",
        "\n",
        "from data_loader import siamese_data_loader\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = 'data/IMFDB_final/', image_list = train_list_file, crop=False, resize = True, resize_shape=[128,128]), batch_size=32, shuffle = False, pin_memory=False)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = 'data/IMFDB_final/', image_list = test_list_file, crop=False, mirror=False, resize = True, resize_shape=[128,128]), batch_size=10, shuffle = False, pin_memory=False)\n",
        "\n",
        "print(f\"The Length of the train loader is {len(trainloader)}\\nThe Length of the test loader is {len(testloader)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of the train loader is 157\n",
            "The Length of the test loader is 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWG1XpBV_CSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d575ea92-7a84-40bf-edde-cfbc2d2d9fc7"
      },
      "source": [
        "classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']\n",
        "num_of_classes = len(classes)\n",
        "print(f\"The Number of classes are: {num_of_classes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Number of classes are: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Aee8StCokK"
      },
      "source": [
        "### Check whether GPU is enabled or not\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0_YCzNqeLyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13619d84-021e-4c51-f491-cf8f74c48b7e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQceFfVvEGdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4a9d9f-422f-402a-86c4-be63531c70d5"
      },
      "source": [
        "from light_cnn import LightCNN_9Layers, network_9layers\n",
        "\n",
        "# Create an object of the LightCNN_9Layers network architecture\n",
        "# The Existing Fc2 Layer outputs 79077 classes, whereas we are dealing with a Multi-Class Classification with 16 number of classes\n",
        "# Update the number of classes in the Fc2 layer to 16 (num_classes), see light_cnn.py for details\n",
        "feature_net = LightCNN_9Layers(num_classes = num_of_classes)\n",
        "feature_net = feature_net.to(device)\n",
        "feature_net"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "network_9layers(\n",
              "  (features): Sequential(\n",
              "    (0): mfm(\n",
              "      (filter): Conv2d(1, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    (2): group(\n",
              "      (conv_a): mfm(\n",
              "        (filter): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (conv): mfm(\n",
              "        (filter): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    (4): group(\n",
              "      (conv_a): mfm(\n",
              "        (filter): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (conv): mfm(\n",
              "        (filter): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    (6): group(\n",
              "      (conv_a): mfm(\n",
              "        (filter): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (conv): mfm(\n",
              "        (filter): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (7): group(\n",
              "      (conv_a): mfm(\n",
              "        (filter): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (conv): mfm(\n",
              "        (filter): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  )\n",
              "  (fc1): mfm(\n",
              "    (filter): Linear(in_features=8192, out_features=512, bias=True)\n",
              "  )\n",
              "  (fc2): Linear(in_features=256, out_features=16, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWjCZhwUynjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088b4def-2a98-4951-adb3-9cb43d217da4"
      },
      "source": [
        "image_1, image_2, label = next(iter(trainloader))\n",
        "image_1.shape, image_2.shape, label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 1, 128, 128]),\n",
              " torch.Size([32, 1, 128, 128]),\n",
              " torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO1HM3Iapc-J"
      },
      "source": [
        "#### Understand the LightCNN Architecture\n",
        "\n",
        "- The LightCNN_9Layers function invokes network_9layers class\n",
        "- The Forward function from network_9layers class takes the input of the network and pass it forwards through its different layers and returns two values, namely:\n",
        "  - Output from the Fc2 (Second fully connected layer that outputs our 16 labels), Fc2_output.\n",
        "    - model(image)[0]\n",
        "  - MFM(Fc1_output)\n",
        "    - Max-FeatureMap (MFM) obtains a compact representation and performs a feature filter selection.\n",
        "    - model(image)[1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzyMeq6NyF7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2cb85d-4c8f-40a1-e157-f26a67e8e000"
      },
      "source": [
        "import inspect\n",
        "\n",
        "# Inspecting the forward function of \"network_9layers\" Class from light_cnn.py\n",
        "print(inspect.getsource(LightCNN_9Layers)) # Print the source code of the LightCNN_9Layers function\n",
        "print(inspect.getsource(network_9layers)) # Print the source code of the network_9layers class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def LightCNN_9Layers(**kwargs):\n",
            "    model = network_9layers(**kwargs)\n",
            "    return model\n",
            "\n",
            "class network_9layers(nn.Module):\n",
            "    def __init__(self, num_classes=79077):\n",
            "        super(network_9layers, self).__init__()\n",
            "        self.features = nn.Sequential(\n",
            "            mfm(1, 48, 5, 1, 2), \n",
            "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True), \n",
            "            group(48, 96, 3, 1, 1), \n",
            "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
            "            group(96, 192, 3, 1, 1),\n",
            "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True), \n",
            "            group(192, 128, 3, 1, 1),\n",
            "            group(128, 128, 3, 1, 1),\n",
            "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
            "            )\n",
            "        self.fc1 = mfm(8*8*128, 256, type=0)\n",
            "        self.fc2 = nn.Linear(256, num_classes)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.features(x)\n",
            "        x = x.view(x.size(0), -1)\n",
            "        x = self.fc1(x)\n",
            "        x = F.dropout(x, training=self.training)\n",
            "        out = self.fc2(x)\n",
            "        return out, x\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSpNHLJ9yp39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cc35b4-e416-433b-d973-1b9a207ee29b"
      },
      "source": [
        "featurenet_output = feature_net(image_1.to(device))\n",
        "print(\"Fc2 Output shape: \", featurenet_output[0].shape)\n",
        "print(\"Max-Feature-Map(Fc1 Output) Output shape: \", featurenet_output[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fc2 Output shape:  torch.Size([32, 16])\n",
            "Max-Feature-Map(Fc1 Output) Output shape:  torch.Size([32, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH4gNjpKWYIQ"
      },
      "source": [
        "### Initializing the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s37QiBqfpD-O"
      },
      "source": [
        "best_loss = 99999999 # Intilizaing the loss value as high value\n",
        "criterion = nn.CrossEntropyLoss() # This loss is used for comparing the labels at the classification stage\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0tOV2O4pD-P"
      },
      "source": [
        "# Implementation of contrastive loss\n",
        "from contrastive import *\n",
        "\n",
        "siamese_loss = contrastive_loss()   # Notice a new loss. contrastive.py shows how to get the loss between the two image features\n",
        "siamese_loss = siamese_loss.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfsv1AzT4aZP"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOmg3Ypb_CTO"
      },
      "source": [
        "#### Training Function\n",
        "\n",
        "Let us train the siamese networks. The objective is images from the same class (positive pair, label = 0) should have similar features and images from different classes (negative pair, label = 1) should have different features. Instead of having two physical networks sharing the weights, in implementation, we have only one network and first-pass image_1 (to get its feature) and then pass image_2 (to get its feature) through the same network. We then compute the contrastive loss on these feature pairs from input image pairs. This saves a lot of memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isK--8bS_CTS"
      },
      "source": [
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    feature_net.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(trainloader):\n",
        "        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        features_1 = feature_net(inputs_1)[1]     # Get feature for image_1\n",
        "        features_2 = feature_net(inputs_2)[1]     # Get feature for image_2\n",
        "        \n",
        "        loss = siamese_loss(features_1, features_2, targets.float())   # Compute the contrastive loss, computes the similarity between the features.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj7vLS23C_SS"
      },
      "source": [
        "#### Testing Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbxvb3UC9_r"
      },
      "source": [
        "Create a directory (checkpoint) to store the trained model when the running loss is less than the best_loss\n",
        "\n",
        "Hint: [Saving and loading model](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWbXaEiTqOvU"
      },
      "source": [
        "if not os.path.isdir(\"data/checkpoint\"):\n",
        "  os.mkdir(\"data/checkpoint\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMwHtczA_CTc"
      },
      "source": [
        "def test(epoch):\n",
        "    global best_loss\n",
        "    feature_net.eval()\n",
        "    test_loss = 0\n",
        "    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n",
        "        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        features_1 = feature_net(inputs_1)[1]     # Get feature for image_1\n",
        "        features_2 = feature_net(inputs_2)[1]     # Get feature for image_2      \n",
        "        \n",
        "        loss = siamese_loss(features_1, features_2, targets.float())\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    # Save checkpoint.\n",
        "    running_loss = test_loss/len(testloader)\n",
        "    if  running_loss < best_loss:   # Save model with the best loss so far\n",
        "        print('Saving the model with the best loss')\n",
        "\n",
        "        # Saving the model as a state dictionary\n",
        "        # A state_dict is simply a Python dictionary object that maps each layer of the network to its parameters (weights)\n",
        "        # As a Python dictionary it can be easily saved, updated, altered, and restored, adding a great deal of modularity to PyTorch models \n",
        "        torch.save(feature_net.state_dict(), 'data/checkpoint/siamese_ckpt.t7')\n",
        "        best_loss = running_loss\n",
        "    \n",
        "    return running_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcuhfqI1wJlA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee208df-f209-462d-d64b-85225ff4209f"
      },
      "source": [
        "optimizer = optim.Adam(feature_net.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n",
        "\n",
        "for epoch in range(0, 10):\n",
        "    train(epoch)\n",
        "    test_loss = test(epoch)\n",
        "    scheduler.step(test_loss)\n",
        "    print(\"Test Loss: \", test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Saving the model with the best loss\n",
            "Test Loss:  3927.933416886763\n",
            "\n",
            "Epoch: 1\n",
            "Saving the model with the best loss\n",
            "Test Loss:  50.2138352394104\n",
            "\n",
            "Epoch: 2\n",
            "Saving the model with the best loss\n",
            "Test Loss:  23.349257737940007\n",
            "\n",
            "Epoch: 3\n",
            "Saving the model with the best loss\n",
            "Test Loss:  20.262716791846536\n",
            "\n",
            "Epoch: 4\n",
            "Saving the model with the best loss\n",
            "Test Loss:  13.91321503465826\n",
            "\n",
            "Epoch: 5\n",
            "Saving the model with the best loss\n",
            "Test Loss:  11.768015493046153\n",
            "\n",
            "Epoch: 6\n",
            "Saving the model with the best loss\n",
            "Test Loss:  9.302082087776878\n",
            "\n",
            "Epoch: 7\n",
            "Saving the model with the best loss\n",
            "Test Loss:  8.272417889941822\n",
            "\n",
            "Epoch: 8\n",
            "Saving the model with the best loss\n",
            "Test Loss:  7.052724895694039\n",
            "\n",
            "Epoch: 9\n",
            "Saving the model with the best loss\n",
            "Test Loss:  6.948004113544117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYMrwcsA_CUG"
      },
      "source": [
        "# After training we load the model that performed the best on test data (avoid picking overfitted model)\n",
        "# we will use the base pre-trained network for feature extraction only. This feature is used to train an MLP classifier.\n",
        "\n",
        "model = LightCNN_9Layers(num_classes = num_of_classes) # Loading the Architecture\n",
        "model.load_state_dict(torch.load('data/checkpoint/siamese_ckpt.t7')) # Loading the weights on the architecture\n",
        "model = model.to(device).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48su1H5b_CUO"
      },
      "source": [
        "Let us see how well does the siamese detect an imposter(dissimilar images). We check whether image_2 is the same individual as image_1 or an imposter. We do this by computing dissimilarity scores between features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSo3BA0a_CUQ"
      },
      "source": [
        "# Extract testloader for a batch_size of 1\n",
        "testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = 'data/IMFDB_final/', image_list = test_list_file, crop=False, mirror=False, \n",
        "                                                             resize = True, resize_shape=[128,128]), batch_size=1, shuffle = False, pin_memory=False)\n",
        "\n",
        "lab = ['same', 'imposter']\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n",
        "        if batch_idx%10 == 0 or int(targets)==0:      # Show every tenth image or if its the same individual\n",
        "            inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
        "            \n",
        "            # Extract features of images inputs_1 and inputs_2\n",
        "            features_1 = model(inputs_1)[1]\n",
        "            features_2 = model(inputs_2)[1]\n",
        "\n",
        "            # Use cosine similarity to measure the similarity between given two images\n",
        "            dissimilarity = torch.nn.functional.cosine_similarity(features_1, features_2).item()\n",
        "\n",
        "            # Both the images are combined horizontally (axis = 1)\n",
        "            img = np.concatenate((inputs_1.data.cpu().numpy()[0][0], inputs_2.data.cpu().numpy()[0][0]), axis = 1)\n",
        "\n",
        "            # Plot the concatenated image\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.text(260, 20, 'Dissimilarity: {:.2f}'.format(dissimilarity), fontsize=20, color='b')     # Similarity score\n",
        "            plt.text(260, 40, \"Ground Truth: {}\".format(lab[int(targets.data[0])]), fontsize=20, color='g')   # Ground truth\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaFP8u0v4kcF"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9di4sNM_CUc"
      },
      "source": [
        "Now we use this network for feature extraction and train an MLP classifier. Feature_net is not updated /train/ tweak after this. We only train the MLP classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTC3qFNJ_CUe"
      },
      "source": [
        "from data_loader import custom_data_loader\n",
        "\n",
        "train_list_file = 'data/IMFDB_train.txt'   # 5000 images for training\n",
        "test_list_file = 'data/IMFDB_test.txt'     # 1095 images for validation\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(custom_data_loader(img_root = 'data/IMFDB_final/', image_list = train_list_file, crop=False,\n",
        "                                                             resize = True, resize_shape=[128,128]), batch_size=32, shuffle = True, pin_memory=False)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = 'data/IMFDB_final/', image_list = test_list_file, crop=False, mirror=False, \n",
        "                                                             resize = True, resize_shape=[128,128]), batch_size=10, shuffle = False, pin_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CspjpLSntwMh"
      },
      "source": [
        "#### Build a Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUxqd0patvYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a838eda-56d7-4bc6-daab-f6e8b38beec3"
      },
      "source": [
        "classifier = nn.Sequential(nn.Linear(256, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
        "                           nn.Linear(32, num_of_classes))\n",
        "\n",
        "classifier = classifier.to(device)\n",
        "print(classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=32, out_features=16, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzioCAuMD4Ab"
      },
      "source": [
        "#### Train the Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tfWqQZB_CUm"
      },
      "source": [
        "def train_classifier(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    classifier.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        features = model(inputs)[1]\n",
        "        outputs = classifier(features)\n",
        "        size_ = outputs.size()\n",
        "        outputs_ = outputs.view(size_[0], num_of_classes)\n",
        "        loss = criterion(outputs_, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.data\n",
        "        _, predicted = torch.max(outputs_.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LHHFG_D8Yv"
      },
      "source": [
        "#### Test the Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srGe92K5_CUu"
      },
      "source": [
        "# When the running accuracy is greater than the best_acc the respective trained model will be stored under the checkpoint folder\n",
        "best_acc = 0\n",
        "\n",
        "def test_classifier(epoch):\n",
        "    global best_acc\n",
        "    classifier.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        features = model(inputs)[1]\n",
        "        \n",
        "        outputs = classifier(features)\n",
        "        size_ = outputs.size()\n",
        "        outputs_ = outputs.view(size_[0], num_of_classes)\n",
        "        loss = criterion(outputs_, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs_.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "               \n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        torch.save(classifier.state_dict(), 'data/checkpoint/classifier_ckpt.t7')\n",
        "        best_acc = acc\n",
        "    \n",
        "    return test_loss/len(testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e3YpfZz_CU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00a6174-6369-469e-ac0f-12c7a4384075"
      },
      "source": [
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   # Dynamic LR scheduler\n",
        "for epoch in range(0, 30):\n",
        "    train_classifier(epoch)\n",
        "    test_loss = test_classifier(epoch)\n",
        "    scheduler.step(test_loss)\n",
        "    print(\"Test Loss: \", test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Saving..\n",
            "Test Loss:  2.486812227422541\n",
            "\n",
            "Epoch: 1\n",
            "Saving..\n",
            "Test Loss:  2.311224819313396\n",
            "\n",
            "Epoch: 2\n",
            "Saving..\n",
            "Test Loss:  2.1608758666298606\n",
            "\n",
            "Epoch: 3\n",
            "Saving..\n",
            "Test Loss:  2.07811236706647\n",
            "\n",
            "Epoch: 4\n",
            "Test Loss:  2.0601431109688497\n",
            "\n",
            "Epoch: 5\n",
            "Saving..\n",
            "Test Loss:  2.003946253386411\n",
            "\n",
            "Epoch: 6\n",
            "Saving..\n",
            "Test Loss:  1.9573845505714416\n",
            "\n",
            "Epoch: 7\n",
            "Saving..\n",
            "Test Loss:  1.9432239147749815\n",
            "\n",
            "Epoch: 8\n",
            "Saving..\n",
            "Test Loss:  1.9393573127009651\n",
            "\n",
            "Epoch: 9\n",
            "Saving..\n",
            "Test Loss:  1.892762705412778\n",
            "\n",
            "Epoch: 10\n",
            "Test Loss:  1.9132358280095187\n",
            "\n",
            "Epoch: 11\n",
            "Saving..\n",
            "Test Loss:  1.9041230743581599\n",
            "\n",
            "Epoch: 12\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Test Loss:  1.9316681731830945\n",
            "\n",
            "Epoch: 13\n",
            "Saving..\n",
            "Test Loss:  1.8715749074112285\n",
            "\n",
            "Epoch: 14\n",
            "Test Loss:  1.9008064074949784\n",
            "\n",
            "Epoch: 15\n",
            "Saving..\n",
            "Test Loss:  1.8977207899093629\n",
            "\n",
            "Epoch: 16\n",
            "Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Test Loss:  1.9051496521993116\n",
            "\n",
            "Epoch: 17\n",
            "Test Loss:  1.893788288940083\n",
            "\n",
            "Epoch: 18\n",
            "Test Loss:  1.9111427025361494\n",
            "\n",
            "Epoch: 19\n",
            "Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Test Loss:  1.8932752018625085\n",
            "\n",
            "Epoch: 20\n",
            "Test Loss:  1.8862300352616743\n",
            "\n",
            "Epoch: 21\n",
            "Test Loss:  1.8916698542508212\n",
            "\n",
            "Epoch: 22\n",
            "Saving..\n",
            "Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Test Loss:  1.886155245520852\n",
            "\n",
            "Epoch: 23\n",
            "Saving..\n",
            "Test Loss:  1.8773805883797732\n",
            "\n",
            "Epoch: 24\n",
            "Test Loss:  1.886796522140503\n",
            "\n",
            "Epoch: 25\n",
            "Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n",
            "Test Loss:  1.8904028123075312\n",
            "\n",
            "Epoch: 26\n",
            "Test Loss:  1.8892798261208967\n",
            "\n",
            "Epoch: 27\n",
            "Test Loss:  1.8800025473941455\n",
            "\n",
            "Epoch: 28\n",
            "Epoch 00029: reducing learning rate of group 0 to 1.5625e-05.\n",
            "Test Loss:  1.897656345909292\n",
            "\n",
            "Epoch: 29\n",
            "Test Loss:  1.9088470236821609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiwJOl40F0Uz"
      },
      "source": [
        "### References\n",
        "\n",
        "1. https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e\n",
        "2. https://www.iflexion.com/blog/face-recognition-algorithms-work/\n",
        "3. http://cvit.iiit.ac.in/projects/IMFDB/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JZfPfrSshl3"
      },
      "source": [
        "# Please answer the questions below to complete the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilfShvKwOiPN"
      },
      "source": [
        "#@title State True or False: The Siamese data loader gives an image and  its label as input to the Siamese network (where the label is of the class that the image belongs to) ? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"False\" #@param [\"\",\"True\",\"False\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r35isHfTVGKc"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6b4eb9-7fd5-42c8-d356-fe4ab7e7ef62"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 10110\n",
            "Date of submission:  15 May 2022\n",
            "Time of submission:  17:07:30\n",
            "View your submissions: https://aiml.iiith.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}